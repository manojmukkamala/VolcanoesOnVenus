---
title: " Venus Volcanoes Data Analysis"
output: 
    html_document:
        code_folding: hide
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)

library(knitr)

library(reticulate)
use_python("/Users/Manoj/anaconda3/bin/python3", required = T)

```

---

Welcome to the __Data Analysis__ section of the project. 
\

---

```{r, fig.align='center', out.width = "800px", out.height = "1500px"}

knitr::include_graphics('volcano_rgb.jpg', dpi = 600)

```

---

In this section, we spend most of the time on 

- Data Cleansing 

- Data Exploration 

- Data Preparation


These are the crucial steps for any machine learning project as hygiene data yields robust models.  

---

In this section, I have used the following python libraries:

- Numpy

- Pandas

- Matplotlib

- Pylab

```{python, echo = FALSE}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pylab import *
import plotly.graph_objects as go
import plotly.offline as py

```

```{python, echo = FALSE}

# Importing the Dataset

train_images = pd.read_csv("/Users/Manoj/R/Volcanoes_train/train_images.csv", header = None)

train_labels = pd.read_csv("/Users/Manoj/R/Volcanoes_train/train_labels.csv")

test_images = pd.read_csv("/Users/Manoj/R/Volcanoes_test/test_images.csv", header = None)

test_labels = pd.read_csv("/Users/Manoj/R/Volcanoes_test/test_labels.csv")

```

---

### Preliminary Analysis
\

Before performing any data cleansing and data transformations, I deployed a basic hand coded logistic regression model on the training dataset to train and test the presence of a volcano in the input image and obtained 

- Training accuracy: 85.742 %

- Test accuracy: 84.12 %

The accuracy is pretty low and the model is suffering from __high bias__ problem. Therefore, getting more data does not help at this stage. 

We can try:

- Data Cleansing

- Try adding new features or polynomial terms

- Try Neural Networks as they are better for image classification than logistic regression.

---

Let us begin with Data Cleansing.


---

### Data Exploration

---

#### Dimensions of Dataset:

```{python, echo = FALSE}

print("Training Data:", train_images.shape)
print("Testing Data:", test_images.shape)

```

---

#### Sample Input Data:

- Each row represents an image and each column represents corresponding pixel value.

---

```{r, echo = FALSE}

train_imgs = py$train_images[0:6, 0:15]
kable(train_imgs[0:6, 0:15])

```

---

#### Sample Output Data:

---

```{r, echo = FALSE}

train_lbls = py$train_labels[0:5, ]
kable(train_lbls[0:5, ])

```

---

#### Distribution of Target Class in Output

```{python, echo = FALSE, fig.align = "center", fig.width = 4, fig.height = 4}

train_labels["Volcano?"].value_counts().plot.bar()
plt.title("Distribution of Target Class in Output")
plt.ylabel("Number of Records")
plt.xlabel("Class")

```

---

#### Creating Dataframes

Let's combine the train and test data into a single dataframe for easier and effective analysis. We will separate the test data later.

---

```{python, echo = FALSE}

df1 = train_images.join(train_labels["Volcano?"])
df1["trn_tst"] = "train"

df2 = test_images.join(test_labels["Volcano?"])
df2["trn_tst"] = "test"

df = pd.concat([df1, df2], ignore_index = True)
del(df1, df2)

df = df.rename(columns = {'Volcano?':'Volcano'})

```

```{python, echo = FALSE}

print("Dimensions of Combined Data Frame:", df.shape)

```

---

#### Sample of DataFrame

---

```{r, echo = FALSE}

rdf <- py$df[0:6, 0:15]

kable(head(rdf[0:6, 0:15]))

```

---

### Corrupted Data

Since the data that we are dealing with is image data, there is a good chance for data corruption and possibly misclassification.

It seems (also mentioned in data dictionary) that few records are corrupted in the input data. Notice the fourth record (index = 3) in the above sample dataframe. The record seems corrupted because it is having a bunch of 0's as the pixel values. 

Let's plot and see a few records and then we will build a work around to find and filter the corrupted records.

```{python, echo = FALSE}

dfs = df.drop(columns = ["Volcano", "trn_tst"])

```

```{python, echo = FALSE, fig.align = "center"}

plt.figure(figsize=(10, 6), dpi=600)

for i in range(0, 15):
    subplot(3, 5, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(np.array(dfs.iloc[i]).reshape(110, 110))
    plt.title("Volcano" if df.iloc[i]["Volcano"] == 1 else "Not Volcano", fontsize = 10)
    
```

__Observations__:

- The fourth record has some data corrupted at the top of the image.

---

#### Pixel Values Distribution

Let's look at the pixel value distributions for these images.

```{python, echo = FALSE, fig.align = "center"}

plt.figure(figsize=(12, 9))

for i in range(0, 15):
    subplot(3, 5, i+1)
    plt.xticks(fontsize = 6)
    plt.yticks(fontsize = 6)
    plt.hist(np.array(dfs.iloc[i]))
    plt.title("Volcano" if df.iloc[i]["Volcano"] == 1 else "Not Volcano", fontsize = 10)

```

__Observations__:

- According to the histograms, for the corrupted image (fourth), the number of pixels whose value is 0 are relatively high.

---

### Analyzing Dark Pixels

First, let's calculate the number of dark pixels (pixel value = 0) per image. Then we will define a threshold to find and filter corrupted records.

```{python, echo = FALSE}

df["DarkPxls"] = sum((dfs == 0), axis = 1)

```

```{python, echo = FALSE}

#Fetch the index of corrupted images. It is useful later.

crpt_id = df[df["DarkPxls"] != 0].index

```

---

#### Statistics and Distribution of Corrupted Pixels

```{python, echo = FALSE}

for i, j in zip(df["DarkPxls"].loc[crpt_id].describe().index.values, df["DarkPxls"].loc[crpt_id].describe().values):
    print(str(i).upper() + " of Blank/Corrupted Pixels " + str(round(j)), "\n")

```

---

__Quartile Plot__

```{python, echo = FALSE, fig.align = "center", fig.width = 4, fig.height = 4}

plt.figure(figsize=(4, 4))

df["DarkPxls"].loc[crpt_id].quantile([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]).plot.bar()

plt.xlabel("Quantile")
plt.title("Quantile Plot for Corrupted Pixels")
plt.show()

```

__Observations__:

- Almost 50% of the corrupted images have all the pixels corrupted.

- 25% of the corrupted images have around 4000 pixels corrupted.

- 10% of the corrupted images have around 2000 pixels corrupted.

---

#### Corrupted Pixels Distribution 

```{python, echo = FALSE, fig.align = "center", fig.width = 4, fig.height = 4}

plt.figure(figsize=(4, 4))

df["DarkPxls"].loc[crpt_id].plot.hist()

plt.title("Histogram of Corrupted Pixels")
plt.xlabel("Corrupted Pixels")
plt.ylabel("Number of Records")
plt.show()

```

__Observations__:

- According to the histogram, most of the corrupted images have lot of corrupted pixels.

---

#### Target Classes in Corrupted Images

```{python, echo = FALSE, fig.align = "center", fig.width = 4, fig.height = 4}

plt.figure(figsize=(4, 4))

df.loc[crpt_id]["Volcano"].value_counts().plot.bar()

plt.title("Classes in Corrupted data")
plt.ylabel("Number of Records")
plt.xlabel("Class")

```


__Observations__:

- Lot of corrupted images do not have volcanoes in them.

- Only a few corrupted records do have volcanoes.

---

#### Corrupted Images with Volcanoes

```{python, echo = FALSE}
vol_crpt_id = df.loc[crpt_id][df.loc[crpt_id]["Volcano"] == 1].index.values

#df.loc[vol_crpt_id].head()
```

```{python, echo = FALSE}

plt.figure(figsize = (8, 8))

j = 1
for i in vol_crpt_id[0:16]:
    subplot(4, 4, j)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(np.array(dfs.loc[i]).reshape(110, 110))
    plt.title(str(df["DarkPxls"].loc[i]) + " Missing Pixels", fontsize = 10)
    j = j + 1
    
```

---

#### Imputing

As we already have a Class Imbalance (very few images with Volcanoes) in the target variable on our original dataset, let's try not to remove the corrupted images that has volcanoes. Instead, let's fill the corrupted pixels in an image with the mean values of the corresponding image. 

But, should we use Row means or Column means?: It seems, for most of the images, the entire column is corrupted. So, let's use row means of image to replace the corrupted pixel.

---

```{python, echo = FALSE}

vol_crpt_imgs = np.array(dfs.loc[vol_crpt_id]).reshape(22, 110, 110)

for i in range(vol_crpt_imgs.shape[0]):
    vol_crpt_imgs[i] = np.array(pd.DataFrame(vol_crpt_imgs[i]).T.replace(0, pd.DataFrame(vol_crpt_imgs[i]).mean(axis = 1).apply(int)).T)
    
```

---

#### Imputing Corrupted Pixels with Row Means

```{python, echo = FALSE}

plt.figure(figsize = (8, 8))

for i in range(vol_crpt_imgs.shape[0]-6):
    subplot(4, 4, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(vol_crpt_imgs[i])
    
```

---

The images doesn't look good. Row means doesn't seem like a great idea. 

Instead let's compute the mean of every pixel for all the images that are not corrupted and use those means to replace the corrupted pixels in corrupted images.


```{python, echo = FALSE}

dfs = pd.concat([dfs, pd.DataFrame(dfs[df["DarkPxls"] == 0].mean().apply(int)).T], ignore_index = True)

dfs.loc[vol_crpt_id] = dfs.loc[vol_crpt_id].replace(0, dfs.loc[9734])

dfs.drop(index = 9734, inplace = True)

```

---

#### Imputing Corrupted Pixels with Pixel Means from all images

```{python, echo = FALSE}
plt.figure(figsize = (8, 8))

j = 1
for i in vol_crpt_id[0:16]:
    subplot(4, 4, j)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(np.array(dfs.loc[i]).reshape(110, 110))
    j = j + 1
```

Definitely not a wonderful improvement, but much better than using row means. Let's stick with these pixel values for corrupted images.

---

#### Corrupted Images without Volcanoes

```{python, echo = FALSE}

novol_crpt_id = df.loc[crpt_id][df.loc[crpt_id]["Volcano"] == 0].index.values

#df.loc[novol_crpt_id].head()

```

Let's plot a few records whose corrupted pixels are within 0.1 Quantile

```{python, echo = FALSE}

plt.figure(figsize = (8, 8))

j = 1
for i in novol_crpt_id[0:16]:
    subplot(4, 4, j)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(np.array(dfs.loc[i]).reshape(110, 110))
    plt.title(str(df["DarkPxls"].loc[i]) + " Missing Pixels", fontsize = 10)
    j = j + 1
    
```

---

As our dataset already have a good number of examples without volcanoes, I think it would not be good idea to invest time to define threshold level for missing pixels or to impute data for corrupted pixels.

Let's omit the corrupted records from the dataset.

---

##### Omitting Corrupted Records with no Volcanoes

```{python, echo = FALSE}

dfs.drop(novol_crpt_id, inplace = True)

```

```{python, echo = FALSE}

print("Shape of New Data Frame:", dfs.shape)

```

```{python, echo = FALSE}

#final_df = dfs.join(df.loc[dfs.index][["Volcano", "trn_tst"]])

```

```{python, echo = FALSE}

#final_df.to_csv("venusvolcanoes.csv", index = False)

```

- Corrupted Records Omitted!